{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15311dde",
   "metadata": {},
   "source": [
    "# Actividad 2: Procesamiento del corpus Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ca54b",
   "metadata": {},
   "source": [
    "En esta segunda tarea cargamos el corpus *Brown* que proporciona *NLTK* para realizar ciertos procesamientos sobre el mismo y comparar el rendimiento de un procesado secuencial frente a un procesado en paralelo con un *Pool* de procesos. Vamos a explicar una a una las diferentes funciones implementadas para realizar dicho procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a342c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from multiprocessing import pool\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964ca07",
   "metadata": {},
   "source": [
    "La función `construye_textos()` simplemente toma las frases contenidas en el corpus *Brown* y genera una permutación aleatoria sobre sus palabras. Los elementos de dicha permutación se unen con espacios mediante `\" \".join()` y se almacenan en una lista.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función toma cada frase del corpus \"brown\" y genera una permutación aletoria\n",
    "# de sus palabras generando como resultado una frase en otro orden distinto al original\n",
    "# Las frases generadas se devuelven en una lista.\n",
    "def construye_textos():\n",
    "    return [\" \".join(np.random.permutation(sents)) for sents in brown.sents()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81de173",
   "metadata": {},
   "source": [
    "La función `reemplazar_comillas(texto)` recibe como argumento una columna del DataFrame Pandas que contien el texto permutado y reemplaza las comillas `` por una comilla doble \". Para ello, se hace uso de la función `apply` y una función `lambda` de tal forma que, para cada línea se aplica dicha transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd67288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función reemplaza las comillas `` por \"\"\n",
    "def reemplazar_comillas(texto):\n",
    "    return texto.apply(lambda text: text.replace(\"``\", '\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0617e5d",
   "metadata": {},
   "source": [
    "La función `to_lowercase(texto)` recibe como argumento una columa del DataFrame Pandas que contiene el texto permutado y pasa todas las palabras a minúsculas. Para ello, de nuevo, hacemos uso de `apply` y una función `lambda` que invoca la función `lower` sobre cada una de las filas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función convierte todas las palabras a minúsculas\n",
    "def to_lowercase(texto):\n",
    "    return texto.apply(lambda text: text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ae634",
   "metadata": {},
   "source": [
    "La función `contar_palabras(texto)` recibe como argumento una columna del DataFrame Pandas que contiene el texto permutado y hace un conteo de todas las palabras que contiene cada fila. Para ello generamos en la variable ``pattern`` un patrón (expresión regular) que aplicamos para particionar cada una de las líneas de dicha columna mediante ``re.split``. Esta última función devuelve una lista sobre la que invocamos `len` para obtener su longitud o lo que es lo mismo, el número de palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función cuenta las palabras de cada fila en el Dataframe\n",
    "# Para ello, utiliza la librería re de expresiones regulares y la función split\n",
    "# para separar cada línea en base a cierto patrón. La función re.split() devuelve\n",
    "# una lista, para obtener el número de palabras usaremos len() sobre dicha lista\n",
    "def contar_palabras(texto):\n",
    "    pattern = r\"(?:\\s+)|(?:,)|(?:\\-)\"\n",
    "    return texto.apply(lambda x: len(re.split(pattern, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa707ea",
   "metadata": {},
   "source": [
    "El procesado completo del DataFrame se realiza desde la función `process_df(df)` que recibe dicho DataFrame como argumento de entrada y devuelve otro DataFrame distinto sobre el que se han realizado todas las transformaciones previamente descritas. Como puede observarse, la función comienza realizando una copia del DataFrame recibido como arugmento para no modificarlo. Sobre la copia generada se reemplazan las comillas, se pasan sus palabras a minúsculas, y se le añade una nueva columna `words` que contiene un conteo de las palabras de cada fila.\n",
    "\n",
    "Finalmente realizamos un procesado adicional en el que eliminamos aquellas filas que contengan texto con un número excesivo o demasiado bajo de palabras. En concreto, eliminaremos aquellas filas que tengan más de 50 o menos de 10 palabras. Para ello, usamos una búsqueda condicional (`res_df[res_df['words'] > 50]` y `res_df[res_df['words'] < 10`) y la función `drop` para eliminaras del nuevo DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función se encarga de procesar el Dataframe en base a las funciones definidas\n",
    "# previamente\n",
    "def process_df(df):\n",
    "    # Generamos una copia del DataFrame para no modificarlo\n",
    "    res_df = df.copy(deep = True)\n",
    "    # Reemplazamos las comillas usando reemplazar_comillas\n",
    "    res_df['text'] = reemplazar_comillas(res_df['text'])\n",
    "    # Pasamos el texto a minúsculas\n",
    "    res_df['text'] = to_lowercase(res_df['text'])\n",
    "    # Construimos una nueva columna en el Dataframe contando el número de palabras\n",
    "    # de cada línea\n",
    "    res_df['words'] = contar_palabras(res_df['text'])\n",
    "    # Vamos a realizar un procesado adicional para textos demasiado largos o demasiado cortos\n",
    "    # Vamos a elminar los textos demasiado largos > 50 palabras\n",
    "    long_text = res_df[res_df['words'] > 50]\n",
    "    res_df.drop(long_text.index, inplace = True)\n",
    "    # Por otra parte vamos a eliminar los textos demasiado cortos < 50 palabras\n",
    "    short_text = res_df[res_df['words'] < 10]\n",
    "    res_df.drop(short_text.index, inplace = True)\n",
    "    # Hemos terminado el procesado y devolvemos el DatFrame\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b50337",
   "metadata": {},
   "source": [
    "En la parte del código que se muestra a continuación realizamos la comparación de los tiempos de ejecución secuenciales con los paralelos. Comenzamos generando un DataFrame con una columna `text` que se obtiene llamando cuatro veces a la función `construye_textos()` y concatenando las listas que devuelve cada una de las llamadas. Una vez generado el Dataframe se realiza una llamada a `process_df()` de forma secuencial (es decir, la función será ejecutada por un solo proceso) y se mide el tiempo de ejecución.\n",
    "\n",
    "A continuación, se repite la tarea pero usando un `Pool` de procesos. Para ello, empezamos creando la variable `NUM_CORES` que contendrá el número de procesos que se asignarán al `Pool`. En este contexto podemos proceder de dos maneras diferentes. Por una parte, podemos hacer uso de la función `cpu_count()` del módulo `multiprocessing` que nos devolvería el número de procesadores lógicos a los que tiene acceso el intérprete de Python. Sin embargo, es posible que este número de procesadores lógicos no se corresponda con el número de procesadores físicos que tiene el sistema en el que se ejecute el código. Podemos optar por tanto, por fijar manualmente la variable `NUM_CORES` al número de procesadores físicos. En el caso que exponemos a continuación se ha optado por esta última opción y se ha fijado `NUM_CORES = 8`. \n",
    "\n",
    "Una vez elegido el número de procesos, se utiliza la función `np.array_split(brown_df, NUM_CORES)` para particionar nuestro DataFrame en tantos *chunks* o segmentos como procesos vayamos a utilizar. De esta forma, garantizamos que a cada proceso se le asigne una parte del DataFrama además de tratar de balancear la carga de trabajo realizando un particionado con el mismo número de filas por segmento. Finalmente, se genera el `Pool`de procesos y se hace uso de la función `pool.map` para que cada proceso aplique la función `process_df()` sobre el segmento que le corresponde. Se hace uso finalmente de la función `concat` de `pandas` para concatenar en un único DataFrame los DataFrames procesados que devuelve cada proceso. \n",
    "\n",
    "El código que aquí se adjunta no puede ser ejecutado en formato *notebook*. Sin embargo, a nivel local se lanzó la ejecución de este script, repiendo un total de 100 veces las tomas de tiempo tanto para el procesado secuencial como para el paralelo. Una vez realizadas todas las medidas, se calculó la media para obtener los siguientes tiempos:\n",
    "\n",
    "- Tiempo medio de ejecución en secuencial: 2.6212 s\n",
    "- Tiempo medio de ejecución en paralelo: 0.76166 s\n",
    "\n",
    "En este caso, podemos observar como obtenemos un tiempo menor utilizando un `Pool` de ocho procesos. Podemos calcular la aceleración la siguiente expresión:\n",
    "\n",
    "$$ Aceleracion = \\frac{T_{ej\\_secuencial}}{T_{ej\\_paralela}} = \\frac{2.6212}{0.76166} = 3.4414 $$\n",
    "\n",
    "Vemos como obtenemos una aceleración del 3.4414 es decir, el proceso paralelizado es 3.4414 veces más rápido que el proceso secuencial. Si bien estamos lejos de obtener una aceleración $n$, es decir, una aceleración igual al número de procesos del `Pool`, sí que obtenemos una aceleración lo suficientemente significativa como para suponer una ventaja frente al procesado en secuencial. De todo esto también puede deducirse que, para el procesamiento de largos conjuntos de datos que contienen lenguaje natural, el paralelizado de dichas tareas puede acelerar significativamente la velocidad de procesado. En nuestro caso, hemos trabajado con un DataFrame de 229360 filas y con 8 procesos. Para DataFrames más grandes (con un número mayor de filas) y con computadores con mayor capadidad de proceso (entendida en este contexto como un número mayor de unidades de proceso) se podría llegar a obtener una aceleración todavía más significativa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos construyendo el DataFrame con el corpus \"BROWN\"\n",
    "# El DataFrame tendrá en principio una única columna 'text' en la que\n",
    "# cada fila será una frase del corpus brown permutada aleatoriamente\n",
    "print(\"Generando nuevo DataFrame...\\n\\n\")\n",
    "brown_df = pd.DataFrame({'text': construye_textos() + construye_textos() + construye_textos() + construye_textos()})\n",
    "print(f\"Generado el nuevo Dataframe -> \\n {brown_df.head()}\\n\\n\")\n",
    "# Comenzamos planteando el procesado del Dataframe en secuencial\n",
    "t0 = time.time()\n",
    "processed_df = process_df(brown_df)\n",
    "print(f\"Tiempo en secuencial -> {time.time() - t0}\")\n",
    "print(f\"{processed_df.head()}\\n\\n\")\n",
    "# Seguimos planteando el procesado del DataFrame de forma paralelizada\n",
    "# Para empezar dividimos el DataFrame en tantos \"chunks\" como procesadores\n",
    "# tenga nuestra máquina. La función multiprocessing.cpu_countS() devuelve el \n",
    "# número de \"procesadores lógicos\" a los que tiene acceso el intérprete de Python.\n",
    "# La máquina en la que se desarrolló este código se contaba con 8 procesadores físicos\n",
    "# por lo que tomaremos este número para NUM_CORES con el objetivo de mejorar el \n",
    "# rendimiento del código paralelizado.\n",
    "NUM_CORES =  8 #multiprocessing.cpu_count()\n",
    "print(f\"Iniciando procesado en paralelo usando un Pool de {NUM_CORES} procesos\\n\\n\")\n",
    "df_chunks = np.array_split(brown_df, NUM_CORES)\n",
    "t0 = time.time()\n",
    "# Cada proceso del Pool procesa una de las partes en las que se ha dividido el DataFrame original.\n",
    "# Finalmente se usa pd.concat() para ir acumulando los resultados que produce cada proceso\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "    processed_df_pool = pd.concat(pool.map(process_df, df_chunks), ignore_index = True)\n",
    "print(f\"Tiempo en paralelo -> {time.time() - t0}\\n\")\n",
    "print(f\"{processed_df_pool.head()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5270c",
   "metadata": {},
   "source": [
    "En este último fragmento de código se muestran algunas de las frases seleccionadas aleatoriamente tanto del DataFrame obtenido de forma secuencial como del obtenido de forma paralela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos algunas frases extraidas aleatoriamente del procesado secuencial\n",
    "print(\"----------------EJEMPLOS DEL PROCESADO SECUENCIAL----------------\")\n",
    "for i in range(5):\n",
    "    print(f\"{processed_df.iloc[np.random.randint(0, len(processed_df))]['text']}\\n\")\n",
    "# Mostramos algunas frases extraidas aleatoriamente del procesado paralelo\n",
    "print(\"----------------EJEMPLOS DEL PROCESADO PARALELO----------------\")\n",
    "for i in range(5):\n",
    "    print(f\"{processed_df_pool.iloc[np.random.randint(0, len(processed_df_pool))]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b934c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
